{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 04: Image Matching and Image Stitching\n",
    "\n",
    "In this lab, you'll implement and play with the algorithms taught in course 5 and 6. \n",
    "\n",
    "- Student Name: 你的名字\n",
    "- Student ID: 你的学号\n",
    "- Date: 2021-10-27"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Image Matching\n",
    "\n",
    "### Task 1: Find corners with harris detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load an example image\n",
    "filename = 'building.jpeg'\n",
    "img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "fig = plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Compute the covariance matrix at each point\n",
    "\n",
    "$$H=\\sum_{(u,v)}w(u,v)\\begin{bmatrix}I_x^2& I_xI_y\\\\I_xI_y &I_y^2\\end{bmatrix}$$\n",
    "\n",
    "where $I_x=\\frac{\\partial f}{\\partial x}, I_y=\\frac{\\partial f}{\\partial y}$ .\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sobel operator at every location\n",
    "\n",
    "# gaussian weights, what window-size should be used?\n",
    "\n",
    "# compute H \n",
    "H = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2. Harris response\n",
    "\n",
    "Theoretically, we can compute eigenvalues\n",
    "\n",
    "$H=\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix}\\quad \\lambda_\\pm=\\frac{1}{2}((a+d)\\pm\\sqrt{4bc+(a-d)^2})$\n",
    "\n",
    "and then classify points using eigenvalues of H, like:\n",
    "\n",
    "<img src=\"https://opencv24-python-tutorials.readthedocs.io/en/latest/_images/harris_region.jpg\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "However, computing eigenvalues are expensive, so we use the following alternative\n",
    "\n",
    "$$f=\\frac{\\lambda_1 \\lambda_2}{\\lambda_1+\\lambda_2}=\\frac{determinant(H)}{trace(H)}$$\n",
    "\n",
    "where $det(\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix})=ad-bc$,   and $trace(\\begin{bmatrix}a&b\\\\c&d\\end{bmatrix})=a+d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute harris response\n",
    "f = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3. Threshold $f$ and visualize\n",
    "\n",
    "we skip non-maximum suppression operation here. You only need to visualize the thresholded harris response map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold and visualize the response map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further reading:\n",
    "\n",
    "[【计算机视觉】2. 特征点检测：Harris, SIFT, SURF, ORB](https://zhuanlan.zhihu.com/p/36382429)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: SIFT\n",
    "\n",
    "To do this task, read [opencv documentation on SIFT](https://docs.opencv.org/4.5.4/d7/d60/classcv_1_1SIFT.html) first, and use it for local feature detection and description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image\n",
    "img0 = cv2.imread(\"1.jpeg\", cv2.IMREAD_GRAYSCALE)\n",
    "img1 = cv2.imread(\"2.jpeg\", cv2.IMREAD_GRAYSCALE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat sift extractor (detector + descriptor)\n",
    "SIFT = ...\n",
    "\n",
    "# get the detector and descriptor\n",
    "kpts0, descs0 = ... \n",
    "kpts1, descs1 = ... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For SIFT descriptors, people usually match them with ratio-test.\n",
    "\n",
    "(1) Please list the main advantage of ratio-test in matching SIFT descriptors.\n",
    "\n",
    "(2) Do you think mutual-nearest-neighbor method can also work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:red\">\n",
    " \n",
    " \n",
    " REPLACE THIS WITH YOUR ANSWER\n",
    " \n",
    " \n",
    " </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute descriptor distance\n",
    "distance = ...\n",
    "\n",
    "# ratio test\n",
    "mkpts0, mkpts1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And visualize the final matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "from utils import make_matching_figure\n",
    "fig = make_matching_figure(...) # You might need to read the documentation of this function. Or you can write your own drawing function.\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Image Stitching\n",
    "\n",
    "One application fo image matching is to stitch multiple images and get one panorama.\n",
    "\n",
    "### Task 3: Transformation\n",
    "\n",
    "Considering 2 images as input, you can use SIFT (provided by cv2) to find the transformation between them (implement it on your own)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load images\n",
    "img0_rgb = cv2.imread(\"1.jpeg\", cv2.IMREAD_COLOR)[..., [2,1,0]]\n",
    "img1_rgb = cv2.imread(\"2.jpeg\", cv2.IMREAD_COLOR)[..., [2,1,0]]\n",
    "img0_gray = cv2.cvtColor(img0, cv2.COLOR_RGB2GRAY)\n",
    "img1_gray = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute SIFT keypoints and descriptors\n",
    "# note: on gray image\n",
    "\n",
    "# find matches\n",
    "mkpts0 = ...\n",
    "mkpts1 = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the transformation $H$ is defined as \n",
    "$$\\begin{bmatrix}x_0\\\\y_0\\\\1\\end{bmatrix}=\\begin{bmatrix}h_{11}&h_{12}&h_{13}\\\\h_{21}&h_{22}&h_{23}\\\\0&0&1\\end{bmatrix}\\begin{bmatrix}x_1\\\\y_1\\\\1\\end{bmatrix}$$\n",
    "\n",
    "Please answer:\n",
    "\n",
    "(1) What type is this transformation?\n",
    "\n",
    "(2) Please write down the converted equation in the form of $Ah=0$. To solve this equation, what's the minimal number of matches that we need? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:red\">\n",
    " \n",
    " \n",
    " REPLACE THIS WITH YOUR ANSWER\n",
    " \n",
    " \n",
    " </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select K matches (according to your answer)\n",
    "selected_mkpts0 = ...\n",
    "selected_mkpts1 = ...\n",
    "\n",
    "# solve the equation with np.linalg.solve(A,0)\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.linalg.solve.html\n",
    "A = ...\n",
    "h = np.linalg.solve(A, np.zeros())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: RANSAC\n",
    "\n",
    "To use naive ransac algorithm, we need $N$ sample-points(样本点), to solve the model, we need $K$ sample-points as a minimal requirement. Then perform:\n",
    "\n",
    "1. Randomly sample $K$ sample-points.\n",
    "2. Fit the model with $K$ sample-points. Denoted as $\\hat h$.\n",
    "3. Compute error of other sample points according to $\\hat h$. Count the inliers within some threshold.\n",
    "4. Repeat $M$ times, the final $h$ is the $\\hat h$ with most inliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement your own RANSAC\n",
    "def ransac_to_estimate_H(samples, K, inlier_thr, M):\n",
    "    ...\n",
    "\n",
    "H = ransac_to_estimate_H()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cv2.warpPerspective to put one image on the other\n",
    "dsize = ...\n",
    "pano = cv2.warpPerspective(img1_rgb, dsize, img0_rgb, cv2.INTER_LINEAR)\n",
    "\n",
    "# visualize the results\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hot to solve the artifacts in the overlapping region? Name 2 possible methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <span style=\"color:red\">\n",
    " \n",
    " \n",
    " WRITE YOUR ANSWER HERE.\n",
    " \n",
    " \n",
    " </span>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "293924669efb73c1897f35f1d85665e0746c797b6fb7c06c32fe806c5b6a9b9a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('cv': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
